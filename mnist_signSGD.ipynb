{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ef56gCUqrdVn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import random; random.seed(42)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "assert tf.executing_eagerly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXNp_25y7JP2"
   },
   "source": [
    "DP-SGD has three privacy-specific hyperparameters and one existing hyperamater that you must tune:\n",
    "\n",
    "1. `l2_norm_clip` (float) - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points. \n",
    "2. `noise_multiplier` (float) - The amount of noise sampled and added to gradients during training. Generally, more noise results in better privacy (often, but not necessarily, at the expense of lower utility).\n",
    "3.   `microbatches` (int) - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility. However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. The average gradient across these multiple training examples is then clipped. The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. The number of microbatches should evenly divide the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVw_r2Mq7ntd"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 125\n",
    "l2_norm_clip = 4\n",
    "noise_multiplier = 4\n",
    "num_microbatches = batch_size\n",
    "learning_rate=1e-3\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1ML23FlueTr"
   },
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "train_data = np.expand_dims(train_data, len(train_data.shape))\n",
    "test_data = np.expand_dims(test_data, len(test_data.shape))\n",
    "\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "assert train_data.min() == 0.\n",
    "assert train_data.max() == 1.\n",
    "assert test_data.min() == 0.\n",
    "assert test_data.max() == 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size refers to private data size\n",
    "public_data, private_data, public_labels, private_labels = \\\n",
    "    train_test_split(train_data, train_labels, test_size=239/240)\n",
    "\n",
    "num_batches = private_data.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 28, 28, 1)\n",
      "(250, 10)\n",
      "(59750, 28, 28, 1)\n",
      "(59750, 10)\n"
     ]
    }
   ],
   "source": [
    "print(public_data.shape)\n",
    "print(public_labels.shape)\n",
    "print(private_data.shape)\n",
    "print(private_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws8-nVuVDgtJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.209% and noise_multiplier = 4 iterated over 4780 steps satisfies differential privacy with eps = 0.179 and delta = 1e-05.\n",
      "The optimal RDP order is 128.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1785377849920692, 128.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    n=private_labels.shape[0], batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqBvjCf5-ZXy"
   },
   "outputs": [],
   "source": [
    "# CNN model from the paper\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "def cnn_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(60, activation='linear', use_bias=False))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.load_weights('mnist_abadi_weights.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_grads_mean_var(public_x, public_y, loss_fn, model):\n",
    "    # x needs to have extra dimension for number of examples,\n",
    "    # even if it's 1 for our case\n",
    "    public_x = np.expand_dims(public_x, axis=1)\n",
    "    # https://math.stackexchange.com/questions/20593/calculate-variance-from-a-stream-of-sample-values\n",
    "    mean_k = None\n",
    "    v_k = None\n",
    "    k = 0\n",
    "    for x, y in zip(public_x, public_y):\n",
    "        k += 1\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss_fn(y, model(x))\n",
    "            grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "        numpy_grad = [t.numpy() for t in grad]\n",
    "        if k == 1:\n",
    "            mean_k = numpy_grad\n",
    "            v_k = [np.zeros(t.shape) for t in numpy_grad]\n",
    "        else:\n",
    "            prev_mean_k = mean_k\n",
    "            mean_k = [mean_k[i] + (t - mean_k[i]) / k for i, t in enumerate(numpy_grad)]\n",
    "            v_k = [v_k[i] +  np.multiply(t - prev_mean_k[i], \n",
    "                                         t - mean_k[i]) \n",
    "                   for i, t in enumerate(numpy_grad)]\n",
    "    unbiased_variance = [t / (k - 1) for t in v_k]\n",
    "    return mean_k, unbiased_variance\n",
    "\n",
    "def evaluate_model(model, loss_fn, x, y):\n",
    "    pred = model(x)\n",
    "    loss = np.mean(loss_fn(y, pred).numpy())\n",
    "    acc = np.mean(tf.keras.metrics.categorical_accuracy(y, pred).numpy())\n",
    "    return (loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65507d16d864cc390ba1764e7cd70e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=50, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 21', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 22', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 23', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 24', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 25', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 26', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 27', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 28', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 29', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 30', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 31', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 32', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 33', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 34', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 35', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 36', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 37', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 38', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 39', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 40', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 41', max=250, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.6716361, 0.7993)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "pretrained_model = cnn_model()\n",
    "pretrained_model.compile(optimizer='adam',\n",
    "                       loss=loss_fn, metrics=['accuracy'])\n",
    "baseline_history = pretrained_model.fit(public_data, public_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0,\n",
    "                    callbacks=[TQDMNotebookCallback(), EarlyStopping(monitor='acc', patience=5)])\n",
    "evaluate_model(pretrained_model, loss_fn, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4iV03VqG1Bo"
   },
   "outputs": [],
   "source": [
    "normal_sgd_model = cnn_model()\n",
    "normal_sgd_optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "normal_sgd_model.compile(optimizer=normal_sgd_optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "sign_sgd_model = cnn_model()\n",
    "sign_sgd_optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "sign_sgd_model.compile(optimizer=sign_sgd_optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "dp_sign_sgd_model = cnn_model()\n",
    "dp_sign_sgd_optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "dp_sign_sgd_model.compile(optimizer=dp_sign_sgd_optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "custom_model = cnn_model()\n",
    "custom_optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "custom_model.compile(optimizer=custom_optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "dpsgd_model = cnn_model()\n",
    "dpsgd_optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "dpsgd_model.compile(optimizer=dpsgd_optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "custom_model_pretrained = cnn_model()\n",
    "custom_model_pretrained.set_weights(pretrained_model.get_weights())\n",
    "custom_optimizer_pretrained = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "custom_model_pretrained.compile(optimizer=custom_optimizer_pretrained, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "dpsgd_model_pretrained = cnn_model()\n",
    "dpsgd_model_pretrained.set_weights(pretrained_model.get_weights())\n",
    "dpsgd_optimizer_pretrained = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "dpsgd_model_pretrained.compile(optimizer=dpsgd_optimizer_pretrained, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c863f20d4e544e58a281345182e39861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779beeac4464455784914a4e890b866a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batch', max=478, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d1495f7bc031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m             grads = dp_sign_sgd_optimizer.compute_gradients(\n\u001b[1;32m     87\u001b[0m                 loss, dp_sign_sgd_model.trainable_weights, gradient_tape=tape)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d1495f7bc031>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m             grads = dp_sign_sgd_optimizer.compute_gradients(\n\u001b[1;32m     87\u001b[0m                 loss, dp_sign_sgd_model.trainable_weights, gradient_tape=tape)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Iterate over epochs.\n",
    "normal_sgd_loss_batches = []\n",
    "normal_sgd_acc_batches = []\n",
    "\n",
    "sign_sgd_loss_batches = []\n",
    "sign_sgd_acc_batches = []\n",
    "\n",
    "dp_sign_sgd_loss_batches = []\n",
    "dp_sign_sgd_acc_batches = []\n",
    "\n",
    "custom_loss_batches = []\n",
    "custom_acc_batches = []\n",
    "\n",
    "dpsgd_loss_batches = []\n",
    "dpsgd_acc_batches = []\n",
    "\n",
    "custom_loss_pretrained_batches = []\n",
    "custom_acc_pretrained_batches = []\n",
    "\n",
    "dpsgd_loss_pretrained_batches = []\n",
    "dpsgd_acc_pretrained_batches = []\n",
    "\n",
    "# Used for picking a random minibatch\n",
    "idx_array = np.arange(private_data.shape[0])\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in tqdm(range(num_batches), desc='Batch', leave=False):\n",
    "        \n",
    "        # Pick a random minibatch\n",
    "        random_idx = np.random.choice(idx_array, batch_size)\n",
    "        x_batch_train = private_data[random_idx]\n",
    "        y_batch_train = private_labels[random_idx]\n",
    "        \n",
    "        ### Normal SGD\n",
    "        \n",
    "        # Evaluate model\n",
    "        loss, acc = evaluate_model(normal_sgd_model, loss_fn, test_data, test_labels)\n",
    "        normal_sgd_loss_batches.append(loss)\n",
    "        normal_sgd_acc_batches.append(acc)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_val = loss_fn(y_batch_train, normal_sgd_model(x_batch_train)) # Compute loss value\n",
    "            \n",
    "        grads = tape.gradient(loss_val, normal_sgd_model.trainable_variables)\n",
    "        normal_sgd_optimizer.apply_gradients(zip(grads, normal_sgd_model.trainable_variables))\n",
    "        \n",
    "        ### Sign SGD\n",
    "        \n",
    "        # Evaluate model\n",
    "        loss, acc = evaluate_model(sign_sgd_model, loss_fn, test_data, test_labels)\n",
    "        sign_sgd_loss_batches.append(loss)\n",
    "        sign_sgd_acc_batches.append(acc)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_val = loss_fn(y_batch_train, sign_sgd_model(x_batch_train)) # Compute loss value\n",
    "            \n",
    "        grads = tape.gradient(loss_val, sign_sgd_model.trainable_variables)\n",
    "        grads = [tf.math.sign(t) for t in grads]\n",
    "        \n",
    "        sign_sgd_optimizer.apply_gradients(zip(grads, sign_sgd_model.trainable_variables))\n",
    "        \n",
    "        ### DP SignSGD\n",
    "        \n",
    "        # Evaluate model\n",
    "        loss, acc = evaluate_model(dp_sign_sgd_model, loss_fn, test_data, test_labels)\n",
    "        dp_sign_sgd_loss_batches.append(loss)\n",
    "        dp_sign_sgd_acc_batches.append(acc)\n",
    "    \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = dp_sign_sgd_model(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = dp_sign_sgd_optimizer.compute_gradients(\n",
    "                loss, dp_sign_sgd_model.trainable_weights, gradient_tape=tape)\n",
    "            print([t.shape for t in grads])\n",
    "\n",
    "        del tape\n",
    "            \n",
    "        grads = [tf.math.sign(t) for t in grads]\n",
    "        print([t.shape for t in grads])\n",
    "        \n",
    "        dp_sign_sgd_optimizer.apply_gradients(zip(grads, dp_sign_sgd_model.trainable_variables))\n",
    "        \n",
    "        ### Normal DPSGD\n",
    "    \n",
    "        # Evaluate DPSGD model\n",
    "        loss, acc = evaluate_model(dpsgd_model, loss_fn, test_data, test_labels)\n",
    "#         print('DPSGD Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "        dpsgd_loss_batches.append(loss)\n",
    "        dpsgd_acc_batches.append(acc)\n",
    "    \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = dpsgd_model(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = dpsgd_optimizer.compute_gradients(loss, dpsgd_model.trainable_weights, gradient_tape=tape)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        dpsgd_optimizer.apply_gradients(grads)\n",
    "        \n",
    "#         ### Normal DPSGD pretrained\n",
    "    \n",
    "#         # Evaluate DPSGD model\n",
    "#         loss, acc = evaluate_model(dpsgd_model_pretrained, loss_fn, test_data, test_labels)\n",
    "# #         print('DPSGD Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "#         dpsgd_loss_pretrained_batches.append(loss)\n",
    "#         dpsgd_acc_pretrained_batches.append(acc)\n",
    "    \n",
    "#         # Open a GradientTape to record the operations run\n",
    "#         # during the forward pass, which enables autodifferentiation.\n",
    "#         with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "#             # Run the forward pass of the layer.\n",
    "#             # The operations that the layer applies\n",
    "#             # to its inputs are going to be recorded\n",
    "#             # on the GradientTape.\n",
    "#             logits = dpsgd_model_pretrained(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "#             # Compute the loss value for this minibatch.\n",
    "#             loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "#             # Use the gradient tape to automatically retrieve\n",
    "#             # the gradients of the trainable variables with respect to the loss.\n",
    "#             grads = dpsgd_optimizer_pretrained.compute_gradients(\n",
    "#                 loss, dpsgd_model_pretrained.trainable_weights, gradient_tape=tape)\n",
    "\n",
    "#         del tape\n",
    "\n",
    "#         # Run one step of gradient descent by updating\n",
    "#         # the value of the variables to minimize the loss.\n",
    "#         dpsgd_optimizer_pretrained.apply_gradients(grads)\n",
    "        \n",
    "#         ### Our custom DPSGD\n",
    "    \n",
    "#         # Evaluate custom model\n",
    "#         loss, acc = evaluate_model(custom_model, loss_fn, test_data, test_labels)\n",
    "# #         print('Custom Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "#         custom_loss_batches.append(loss)\n",
    "#         custom_acc_batches.append(acc)\n",
    "\n",
    "#         means, variances = get_public_grads_mean_var(public_data, public_labels, loss_fn, custom_model)\n",
    "#         variances = [layer * public_var_multiplier for layer in variances]\n",
    "        \n",
    "#         # Open a GradientTape to record the operations run\n",
    "#         # during the forward pass, which enables autodifferentiation.\n",
    "#         with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "#             # Run the forward pass of the layer.\n",
    "#             # The operations that the layer applies\n",
    "#             # to its inputs are going to be recorded\n",
    "#             # on the GradientTape.\n",
    "#             logits = custom_model(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "#             # Compute the loss value for this minibatch.\n",
    "#             loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "#             # Use the gradient tape to automatically retrieve\n",
    "#             # the gradients of the trainable variables with respect to the loss.\n",
    "#             grads = custom_optimizer.compute_gradients(loss, custom_model.trainable_weights,\n",
    "#                                                               gradient_tape=tape)\n",
    "\n",
    "#         del tape\n",
    "        \n",
    "#         # X = N(means, variances)\n",
    "#         # Y = X + N(0, gaussian_noise_var)\n",
    "#         # MLE of X is ((variances * Y) + (gaussian_noise_var * means)) / (variances + gaussian_noise_var)\n",
    "#         # https://www.wolframalpha.com/input/?i=differentiate+-log%28%CF%83%29+-+1%2F2+log%282+%CF%80%29+-+1%2F2+%28%28x+-+%CE%BC%29%2F%CF%83%29%5E2+-log%28%CE%A3%29+-+1%2F2+log%282+%CF%80%29+-+1%2F2+%28%28y+-+x%29%2F%CE%A3%29%5E2+wrt+x\n",
    "#         # https://www.wolframalpha.com/input/?i=solve+%28y+-+x%29%2F%CE%A3%5E2+-+%28x+-+%CE%BC%29%2F%CF%83%5E2+for+x\n",
    "#         Ys = [grad[0] for grad in grads]\n",
    "#         Xs = [((Y * variances[i]) + (means[i] * gaussian_noise_var)) / (variances[i] + gaussian_noise_var)\n",
    "#               for i, Y in enumerate(Ys)]\n",
    "#         adjusted_grads = list(zip(Xs, custom_model.trainable_weights))\n",
    "\n",
    "#         # Run one step of gradient descent by updating\n",
    "#         # the value of the variables to minimize the loss.\n",
    "#         custom_optimizer.apply_gradients(adjusted_grads)\n",
    "        \n",
    "#         ### Our custom DPSGD pretrained\n",
    "    \n",
    "#         # Evaluate custom model\n",
    "#         loss, acc = evaluate_model(custom_model_pretrained, loss_fn, test_data, test_labels)\n",
    "# #         print('Custom Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "#         custom_loss_pretrained_batches.append(loss)\n",
    "#         custom_acc_pretrained_batches.append(acc)\n",
    "\n",
    "#         means, variances = get_public_grads_mean_var(public_data, public_labels, loss_fn, custom_model_pretrained)\n",
    "#         variances = [layer * public_var_multiplier for layer in variances]\n",
    "        \n",
    "#         # Open a GradientTape to record the operations run\n",
    "#         # during the forward pass, which enables autodifferentiation.\n",
    "#         with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "#             # Run the forward pass of the layer.\n",
    "#             # The operations that the layer applies\n",
    "#             # to its inputs are going to be recorded\n",
    "#             # on the GradientTape.\n",
    "#             logits = custom_model_pretrained(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "#             # Compute the loss value for this minibatch.\n",
    "#             loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "#             # Use the gradient tape to automatically retrieve\n",
    "#             # the gradients of the trainable variables with respect to the loss.\n",
    "#             grads = custom_optimizer_pretrained.compute_gradients(\n",
    "#                     loss, custom_model_pretrained.trainable_weights, gradient_tape=tape)\n",
    "\n",
    "#         del tape\n",
    "        \n",
    "#         # X = N(means, variances)\n",
    "#         # Y = X + N(0, gaussian_noise_var)\n",
    "#         # MLE of X is ((variances * Y) + (gaussian_noise_var * means)) / (variances + gaussian_noise_var)\n",
    "#         # https://www.wolframalpha.com/input/?i=differentiate+-log%28%CF%83%29+-+1%2F2+log%282+%CF%80%29+-+1%2F2+%28%28x+-+%CE%BC%29%2F%CF%83%29%5E2+-log%28%CE%A3%29+-+1%2F2+log%282+%CF%80%29+-+1%2F2+%28%28y+-+x%29%2F%CE%A3%29%5E2+wrt+x\n",
    "#         # https://www.wolframalpha.com/input/?i=solve+%28y+-+x%29%2F%CE%A3%5E2+-+%28x+-+%CE%BC%29%2F%CF%83%5E2+for+x\n",
    "#         Ys = [grad[0] for grad in grads]\n",
    "#         Xs = [((Y * variances[i]) + (means[i] * gaussian_noise_var)) / (variances[i] + gaussian_noise_var)\n",
    "#               for i, Y in enumerate(Ys)]\n",
    "#         adjusted_grads = list(zip(Xs, custom_model_pretrained.trainable_weights))\n",
    "\n",
    "#         # Run one step of gradient descent by updating\n",
    "#         # the value of the variables to minimize the loss.\n",
    "#         custom_optimizer_pretrained.apply_gradients(adjusted_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(normal_sgd_model, loss_fn, test_data, test_labels))\n",
    "print(evaluate_model(custom_model, loss_fn, test_data, test_labels))\n",
    "print(evaluate_model(dpsgd_model, loss_fn, test_data, test_labels))\n",
    "print(evaluate_model(custom_model_pretrained, loss_fn, test_data, test_labels))\n",
    "print(evaluate_model(dpsgd_model_pretrained, loss_fn, test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "#                         'custom_loss': custom_loss_batches,\n",
    "#                         'dpsgd_loss': dpsgd_loss_batches,\n",
    "                        'non-dp_acc': normal_sgd_acc_batches,\n",
    "                        'signsgd_acc': sign_sgd_acc_batches,\n",
    "                        'dp-signsgd_acc': dp_sign_sgd_acc_batches,\n",
    "#                         'custom_acc': custom_acc_batches,\n",
    "                        'dpsgd_acc': dpsgd_acc_batches,\n",
    "#                         'custom_pretrained_acc': custom_acc_pretrained_batches,\n",
    "#                         'dpsgd_pretrained_acc': dpsgd_acc_pretrained_batches,\n",
    "                       })\n",
    "print(metrics)\n",
    "metrics.to_pickle('mnist_abadi_metrics_signSGD_dpsgd-norm{}_public{}.png'\n",
    "                    .format(l2_norm_clip, public_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16, 10)})\n",
    "ax = sns.lineplot(data=metrics[:200])\n",
    "ax.set(ylim=(0,1))\n",
    "ax.set(xlabel='Minibatch', ylabel='Acc', \n",
    "       title='MNIST Per Weight Bayesian (Norm Clip={}, Public Size={})'.format(l2_norm_clip, public_data.shape[0]))\n",
    "plt.savefig('mnist_abadi_metrics_signSGD_dpsgd-norm{}_public{}.png'\n",
    "                    .format(l2_norm_clip, public_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification_Privacy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
