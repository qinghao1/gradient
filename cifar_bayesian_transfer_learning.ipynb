{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ef56gCUqrdVn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import optimize\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import random; random.seed(42)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "assert tf.executing_eagerly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E14tL1vUuTRV"
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXNp_25y7JP2"
   },
   "source": [
    "DP-SGD has three privacy-specific hyperparameters and one existing hyperamater that you must tune:\n",
    "\n",
    "1. `l2_norm_clip` (float) - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points. \n",
    "2. `noise_multiplier` (float) - The amount of noise sampled and added to gradients during training. Generally, more noise results in better privacy (often, but not necessarily, at the expense of lower utility).\n",
    "3.   `microbatches` (int) - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility. However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. The average gradient across these multiple training examples is then clipped. The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. The number of microbatches should evenly divide the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVw_r2Mq7ntd"
   },
   "outputs": [],
   "source": [
    "l2_norm_clip = 0.5\n",
    "noise_multiplier = 1\n",
    "gaussian_stdev = l2_norm_clip * noise_multiplier\n",
    "gaussian_noise_var = gaussian_stdev ** 2\n",
    "public_var_multiplier = 1 # Multiply public variance by this multiplier\n",
    "public_stdev_multipler = math.sqrt(public_var_multiplier)\n",
    "num_microbatches = batch_size\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1ML23FlueTr"
   },
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.cifar10.load_data()\n",
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "assert train_data.min() == 0.\n",
    "assert train_data.max() == 1.\n",
    "assert test_data.min() == 0.\n",
    "assert test_data.max() == 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size refers to private data size\n",
    "public_data, private_data, public_labels, private_labels = \\\n",
    "    train_test_split(train_data, train_labels, test_size=199/200)\n",
    "\n",
    "num_batches = private_data.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 32, 32, 3)\n",
      "(250, 10)\n",
      "(49750, 32, 32, 3)\n",
      "(49750, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(public_data.shape)\n",
    "print(public_labels.shape)\n",
    "print(private_data.shape)\n",
    "print(private_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws8-nVuVDgtJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.251% and noise_multiplier = 1 iterated over 398 steps satisfies differential privacy with eps = 1.18 and delta = 1e-05.\n",
      "The optimal RDP order is 11.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.177695375447719, 11.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    n=private_labels.shape[0], batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqBvjCf5-ZXy"
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# ~3k parameters, ~80% accuracy\n",
    "def cnn_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=train_data.shape[1:], trainable=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), trainable=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', trainable=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), trainable=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, trainable=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64, trainable=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.save_weights('cifar10_pretrained_weights.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,279,210\n",
      "Trainable params: 650\n",
      "Non-trainable params: 1,278,560\n",
      "_________________________________________________________________\n",
      "[(64, 10), (10,)]\n"
     ]
    }
   ],
   "source": [
    "cnn_model().summary()\n",
    "print([t.numpy().shape for t in cnn_model().trainable_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def regress_all(arr):\n",
    "    # Add 1s for bias\n",
    "    m, n = arr.shape\n",
    "    arr = np.insert(arr, arr.shape[1], 1, axis=1)\n",
    "    all_coeffs = []\n",
    "    for i in range(n):\n",
    "        coeff_mask = np.ones(arr.shape, dtype=np.bool)\n",
    "        coeff_mask[:, i] = False\n",
    "        dependant_mask = np.zeros(arr.shape, dtype=np.bool)\n",
    "        dependant_mask[:, i] = True\n",
    "        coeffs = scipy.linalg.lstsq(arr[coeff_mask].reshape((m, n)), arr[dependant_mask])[0]\n",
    "        coeffs = np.insert(coeffs, i, 0)\n",
    "        coeffs = np.delete(coeffs, -1) # Delete bias\n",
    "        all_coeffs.append(coeffs)\n",
    "    return all_coeffs\n",
    "\n",
    "def construct_graphical_model_and_get_map(public_grads, dp_grads):\n",
    "    public_grads_means = [np.mean(l, axis=0) for l in public_grads]\n",
    "    public_grads_stdev = [np.std(l, axis=0, ddof=1) * public_stdev_multipler\n",
    "                          for l in public_grads]\n",
    "    public_weights = public_grads[0]\n",
    "    observed_weights = dp_grads[0]\n",
    "    observed_biases = dp_grads[1]\n",
    "    \n",
    "    regress_coeffs = []\n",
    "    for i in range(public_weights.shape[2]):\n",
    "        regress_coeffs.append(regress_all(public_weights[:, :, i]))\n",
    "    regress_coeffs = np.asarray(regress_coeffs)\n",
    "    \n",
    "    def log_likelihood(X):\n",
    "        X = X.reshape(observed_weights.shape)\n",
    "        ll = np.zeros(X.shape)\n",
    "        # (10, 64, 64) @ (64, 10) -> (64, 10)\n",
    "        res = np.zeros(X.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                res[i, j] = np.dot(regress_coeffs[j, i], X[:, j])\n",
    "        # Prior using public mean and stdev\n",
    "        ll += scipy.stats.norm.logpdf(X, loc=public_grads_means[0] + res, \n",
    "                                      scale=public_grads_stdev[0])\n",
    "        # Emission probability\n",
    "        ll += scipy.stats.norm.logpdf(X-observed_weights, loc=0, scale=gaussian_stdev)\n",
    "        return -np.sum(ll)\n",
    "\n",
    "    map_weights = optimize.minimize(log_likelihood, np.zeros(observed_weights.shape), method='L-BFGS-B').x\n",
    "    map_weights = map_weights.reshape(observed_weights.shape)\n",
    "    public_bias_var = public_grads_stdev[0] ** 2\n",
    "    map_biases = ((observed_biases * public_bias_var) + (public_grads_means[0] * gaussian_noise_var)) / \\\n",
    "                    (public_bias_var + gaussian_noise_var)\n",
    "    \n",
    "    return [map_weights, map_biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(construct_graphical_model_and_get_map(public_grads, dp_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_grads(public_x, public_y, loss_fn, model):\n",
    "    public_grads = []\n",
    "    # x needs to have extra dimension for number of examples,\n",
    "    # even if it's 1 for our case\n",
    "    public_x = np.expand_dims(public_x, axis=1)\n",
    "    for x, y in zip(public_x, public_y):\n",
    "#     for x, y in tqdm(zip(public_x, public_y), total=public_x.shape[0], desc='Public Dataset Iter'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss_fn(y, model(x))\n",
    "            grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "        if not public_grads:\n",
    "            public_grads = [[] for _ in grad]\n",
    "        for i, t in enumerate(grad):\n",
    "            public_grads[i].append(t.numpy())\n",
    "    public_grads = [np.asarray(l) for l in public_grads]\n",
    "    return public_grads\n",
    "\n",
    "def get_public_grads_mean_var(public_x, public_y, loss_fn, model):\n",
    "    # x needs to have extra dimension for number of examples,\n",
    "    # even if it's 1 for our case\n",
    "    public_x = np.expand_dims(public_x, axis=1)\n",
    "    # https://math.stackexchange.com/questions/20593/calculate-variance-from-a-stream-of-sample-values\n",
    "    mean_k = None\n",
    "    v_k = None\n",
    "    k = 0\n",
    "    for x, y in zip(public_x, public_y):\n",
    "        k += 1\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss_fn(y, model(x))\n",
    "            grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "        numpy_grad = [t.numpy() for t in grad]\n",
    "        if k == 1:\n",
    "            mean_k = numpy_grad\n",
    "            v_k = [np.zeros(t.shape) for t in numpy_grad]\n",
    "        else:\n",
    "            prev_mean_k = mean_k\n",
    "            mean_k = [mean_k[i] + (t - mean_k[i]) / k for i, t in enumerate(numpy_grad)]\n",
    "            v_k = [v_k[i] +  np.multiply(t - prev_mean_k[i], \n",
    "                                         t - mean_k[i]) \n",
    "                   for i, t in enumerate(numpy_grad)]\n",
    "    unbiased_variance = [t / (k - 1) for t in v_k]\n",
    "    return mean_k, unbiased_variance\n",
    "\n",
    "def evaluate_model(model, loss_fn, x, y):\n",
    "    pred = model(x)\n",
    "    loss = np.mean(loss_fn(y, pred).numpy())\n",
    "    acc = np.mean(tf.keras.metrics.categorical_accuracy(y, pred).numpy())\n",
    "    return (loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c31ea1449641f2a3ec4866a5793d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=50, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.3029099, 0.0978)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "pretrained_model = cnn_model()\n",
    "pretrained_model.compile(optimizer='adam',\n",
    "                       loss=loss_fn, metrics=['accuracy'])\n",
    "baseline_history = pretrained_model.fit(public_data, public_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0,\n",
    "                    callbacks=[TQDMNotebookCallback(), EarlyStopping(monitor='acc', patience=5)])\n",
    "evaluate_model(pretrained_model, loss_fn, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4iV03VqG1Bo"
   },
   "outputs": [],
   "source": [
    "dpsgd_model_pretrained = cnn_model()\n",
    "dpsgd_model_pretrained.set_weights(pretrained_model.get_weights())\n",
    "dpsgd_optimizer_pretrained = DPAdamGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches)\n",
    "dpsgd_model_pretrained.compile(optimizer=dpsgd_optimizer_pretrained, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "bayesian_model_pretrained = cnn_model()\n",
    "bayesian_model_pretrained.set_weights(pretrained_model.get_weights())\n",
    "bayesian_optimizer_pretrained = DPAdamGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches)\n",
    "bayesian_model_pretrained.compile(optimizer=bayesian_optimizer_pretrained, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "bayesian_network_model_pretrained = cnn_model()\n",
    "bayesian_network_model_pretrained.set_weights(pretrained_model.get_weights())\n",
    "bayesian_network_optimizer_pretrained = DPAdamGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches)\n",
    "bayesian_network_model_pretrained.compile(\n",
    "    optimizer=bayesian_network_optimizer_pretrained, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa3eecf17064a58a2aec4961002d6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a775e74a7f14205ba43f0a2bba6295b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batch', max=398, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:From /home/qinghao/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:562: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "WARNING:tensorflow:From /home/qinghao/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qinghao/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1762: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "/home/qinghao/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:897: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "/home/qinghao/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:897: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "/home/qinghao/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1762: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over epochs.\n",
    "dpsgd_loss_pretrained_batches = []\n",
    "dpsgd_acc_pretrained_batches = []\n",
    "\n",
    "bayesian_loss_pretrained_batches = []\n",
    "bayesian_acc_pretrained_batches = []\n",
    "\n",
    "bayesian_network_loss_pretrained_batches = []\n",
    "bayesian_network_acc_pretrained_batches = []\n",
    "\n",
    "# Used for picking a random minibatch\n",
    "idx_array = np.arange(private_data.shape[0])\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in tqdm(range(num_batches), desc='Batch', leave=False):\n",
    "        \n",
    "        # Pick a random minibatch\n",
    "        random_idx = np.random.choice(idx_array, batch_size)\n",
    "        x_batch_train = private_data[random_idx]\n",
    "        y_batch_train = private_labels[random_idx]\n",
    "        \n",
    "        ### DPSGD pretrained\n",
    "    \n",
    "        # Evaluate DPSGD model\n",
    "        loss, acc = evaluate_model(dpsgd_model_pretrained, loss_fn, test_data, test_labels)\n",
    "#         print('DPSGD Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "        dpsgd_loss_pretrained_batches.append(loss)\n",
    "        dpsgd_acc_pretrained_batches.append(acc)\n",
    "    \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = dpsgd_model_pretrained(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = dpsgd_optimizer_pretrained.compute_gradients(\n",
    "                loss, dpsgd_model_pretrained.trainable_weights, gradient_tape=tape)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        dpsgd_optimizer_pretrained.apply_gradients(grads)\n",
    "        \n",
    "        ### Our simple Bayesian DPSGD pretrained\n",
    "    \n",
    "        # Evaluate custom model\n",
    "        loss, acc = evaluate_model(bayesian_model_pretrained, loss_fn, test_data, test_labels)\n",
    "#         print('Custom Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "        bayesian_loss_pretrained_batches.append(loss)\n",
    "        bayesian_acc_pretrained_batches.append(acc)\n",
    "\n",
    "        means, variances = get_public_grads_mean_var(public_data, public_labels, loss_fn, bayesian_model_pretrained)\n",
    "        variances = [layer * public_var_multiplier for layer in variances]\n",
    "        \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = bayesian_model_pretrained(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = bayesian_optimizer_pretrained.compute_gradients(\n",
    "                    loss, bayesian_model_pretrained.trainable_weights, gradient_tape=tape)\n",
    "\n",
    "        del tape\n",
    "        \n",
    "        # X = N(means, variances)\n",
    "        # Y = X + N(0, gaussian_noise_var)\n",
    "        # MLE of X is ((variances * Y) + (gaussian_noise_var * means)) / (variances + gaussian_noise_var)\n",
    "        # https://www.wolframalpha.com/input/?i=differentiate+-log%28%CF%83%29+-+1%2F2+log%282+%CF%80%29+-+1%2F2+%28%28x+-+%CE%BC%29%2F%CF%83%29%5E2+-log%28%CE%A3%29+-+1%2F2+log%282+%CF%80%29+-+1%2F2+%28%28y+-+x%29%2F%CE%A3%29%5E2+wrt+x\n",
    "        # https://www.wolframalpha.com/input/?i=solve+%28y+-+x%29%2F%CE%A3%5E2+-+%28x+-+%CE%BC%29%2F%CF%83%5E2+for+x\n",
    "        Ys = [grad[0] for grad in grads]\n",
    "        Xs = [((Y * variances[i]) + (means[i] * gaussian_noise_var)) / (variances[i] + gaussian_noise_var)\n",
    "              for i, Y in enumerate(Ys)]\n",
    "        adjusted_grads = list(zip(Xs, bayesian_model_pretrained.trainable_weights))\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        bayesian_optimizer_pretrained.apply_gradients(adjusted_grads)\n",
    "        \n",
    "        \n",
    "        ### Our Bayesian graph DPSGD pretrained\n",
    "    \n",
    "        # Evaluate custom model\n",
    "        loss, acc = evaluate_model(bayesian_network_model_pretrained, loss_fn, test_data, test_labels)\n",
    "#         print('Custom Loss: %.4f | Acc: %.4f' % (loss, acc))\n",
    "        bayesian_network_loss_pretrained_batches.append(loss)\n",
    "        bayesian_network_acc_pretrained_batches.append(acc)\n",
    "        \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = bayesian_network_model_pretrained(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss = lambda: loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = bayesian_network_optimizer_pretrained.compute_gradients(\n",
    "                                                            loss,\n",
    "                                                            bayesian_network_model_pretrained.trainable_weights,\n",
    "                                                            gradient_tape=tape)\n",
    "\n",
    "        del tape\n",
    "        \n",
    "        public_grads = get_public_grads(public_data, public_labels, loss_fn, bayesian_model_pretrained)\n",
    "        dp_grads = [g[0].numpy() for g in grads]\n",
    "        map_grads = construct_graphical_model_and_get_map(public_grads, dp_grads)\n",
    "        \n",
    "        bayesian_network_optimizer_pretrained.apply_gradients(\n",
    "            zip(map_grads, bayesian_network_model_pretrained.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(bayesian_network_model, loss_fn, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dpsgd_model, loss_fn, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "                        'dpsgd_acc': dpsgd_acc_pretrained_batches,\n",
    "                        'simple_bayesian_acc': bayesian_acc_pretrained_batches,\n",
    "                        'bayesian_network_acc': bayesian_network_acc_pretrained_batches,\n",
    "                       })\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16, 10)})\n",
    "ax = sns.lineplot(data=metrics)\n",
    "ax.set(xlabel='Minibatch', ylabel='Acc', \n",
    "       title='Pretrained CIFAR10 Bayesian DPSGD (Norm Clip={}, Public Size={})'.format(l2_norm_clip, public_data.shape[0]))\n",
    "plt.savefig('cifar_bayesian_network_variance{}_dpsgd-norm{}.png'\n",
    "            .format(public_var_multiplier, l2_norm_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification_Privacy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
